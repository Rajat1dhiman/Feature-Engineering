{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Assignment Questions\n"
      ],
      "metadata": {
        "id": "mfBCzH9bn-2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "\n",
        "Ans. A **parameter** is a value or variable that you pass into a function, method, or system to customize how it behaves. Think of it like an input or setting that controls how something works.\n",
        "\n",
        "**In Programming**:\n",
        "\n",
        " * A parameter is defined in the function definition.\n",
        " * An argument is the actual value you pass when calling the function."
      ],
      "metadata": {
        "id": "Whohc2p-oCmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is correlation?\n",
        "\n",
        "   What does negative correlation mean?\n",
        "\n",
        "Ans. **Correlation** is a statistical measure that shows how two variables move in relation to each other.\n",
        "\n",
        "    * If both variables tend to increase or decrease together, they are **positively correlated**.\n",
        "    * If one increases while the other decreases, they are **negatively correlated**.\n",
        "    * If there is no consistent pattern, they have **no correlation**.\n",
        "\n",
        "A **negative correlation** means that as **one variable increases, the other decreases**.\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "* The more time you spend watching TV, the **lower** your grades might be.\n",
        "\n",
        "* The **more** you drive the car, the less fuel you have left in the tank."
      ],
      "metadata": {
        "id": "7FtCV81Qozr3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define Machine Learning. What are the main components in Machine learning?\n",
        "\n",
        "Ans. **Machine Learning(ML)** is a branch of artificial intelligence(AI) that allows computers to learn from data and make decisions or predictions without being explicitly programmed for every task.\n",
        "\n",
        "It is like teaching a computer to recognize patterns- the more data it sees, the better it gets at figuring things out.\n",
        "\n",
        "**Main components of ML:**\n",
        "\n",
        "1. **Data**\n",
        "\n",
        "* The most important ingredient.\n",
        "* ML models learn from **examples**, so more data usually means better learning.\n",
        "* Can be images , numbers, text, or anything else.\n",
        "\n",
        "2. **Model(Algorithm)**\n",
        "\n",
        "* The **mathematical structure** or formula that learns from data.\n",
        "* Examples: Linear Regression, Decision Trees, neural Networks.\n",
        "\n",
        "3. **Features**\n",
        "\n",
        "* The **input variables** used to make predictions.\n",
        "* If you are predicting house prices, features might be size, location, number of rooms, etc.\n",
        "\n",
        "4. **Labels(for Supervised Learning)**\n",
        "\n",
        "* The **correct answers** used during training.\n",
        "* If you are training a model to recognize cats vs dogs, the labels would tell the model, \" this is  a cat\" or \" this is a dog\".\n",
        "\n",
        "5. **Training**\n",
        "\n",
        "* The process of **feeding data into the model** so it can learn.\n",
        "* During training, the model adjusts its internal parameters to reduce errors.\n",
        "\n",
        "6. **Evaluation**\n",
        "\n",
        "* Testing how well the model performs on **new, unseen data**.\n",
        "* Helps check if the model is actually learning or just memorizing.\n",
        "\n",
        "7. **Prediction(or Inference)**\n",
        "\n",
        "* After training, the model can make **predictions** or decisions on new data.\n",
        "\n",
        "8. **Loss Function**\n",
        "\n",
        "* A formula that tells the model **how wrong** its predictions are.\n",
        "* The model tries to minimize this loss during training.\n"
      ],
      "metadata": {
        "id": "1NXxn5aMqM6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "Ans. The **loss value** is a number that tells you **how far off** your models's predictions are from the actual answers.\n",
        "\n",
        "* **Low Loss**= model is making good predictions.\n",
        "\n",
        "* **High Loss**= model is making poor predictions\n",
        "\n",
        "During training , the model **tries to minimize the loss**. It adjusts its internal settings to make better predictions.\n",
        "\n",
        "* The **goal** is to find the model configuration that gives the smallest possible loss.\n",
        "* This means the model is making fewer mistakes on the training data."
      ],
      "metadata": {
        "id": "JUgbYWZwtm6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continous and categorical variables?\n",
        "\n",
        "Ans. **Continous Variables**\n",
        "\n",
        "* These are **numerical values** that can take any value within a range- including decimals.\n",
        "* Examples:\n",
        "**Height, temperature age etc.**\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "* These are variables that represent categories, group or labels- they usually have a fixed number of values.\n",
        "\n",
        "* Examples:\n",
        "**Gender, Color, City**\n",
        "\n",
        "Two types:\n",
        "\n",
        "* **Nominal:** No natural order\n",
        "* **Ordinal**: Has an order."
      ],
      "metadata": {
        "id": "HJA1KEdkvGGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What do we handle categorical variables in Machine Learning? What are common\n",
        "techniques?\n",
        "\n",
        "Ans. Most machine learning models (especially ones like linear regression, SVMs, or neural networks) only work with numbers â€” so we canâ€™t feed them raw text like \"Red\", \"Blue\", or \"New York\".\n",
        "\n",
        "** Solution**: We encode these values into numerical formats.\n",
        "\n",
        "**Common Techniques to Handle Categorical Variables**\n",
        "\n",
        "1. **Label Encoding**\n",
        "\n",
        "Assigns a unique integer to each category.\n",
        "\n",
        "\n",
        "Fruit\tEncoded\n",
        "Apple\t0\n",
        "Banana\t1\n",
        "Orange\t2\n",
        "Best for: Ordinal data (where order matters)\n",
        "Be careful: For nominal data (like colors), models might assume 0 < 1 < 2 means something.\n",
        "\n",
        "2. **One-Hot Encoding**\n",
        "\n",
        "Creates binary columns for each category (1 if true, 0 if not).\n",
        "\n",
        "\n",
        "Fruit\tApple\tBanana\tOrange\n",
        "Apple\t1\t0\t0\n",
        "Banana\t0\t1\t0\n",
        "Orange\t0\t0\t1\n",
        " Best for: Nominal data (no order)\n",
        "Drawback: Can create too many columns if the category has many unique values.\n",
        "\n",
        "3. **Ordinal Encoding**\n",
        "Like label encoding but used only when categories have a natural order.\n",
        "\n",
        "\n",
        "Size\tEncoded\n",
        "Small\t0\n",
        "Medium\t1\n",
        "Large\t2\n",
        "Best for: Ordered data like \"low\", \"medium\", \"high\"\n",
        "\n",
        "4. **Target Encoding (Mean Encoding)**\n",
        "\n",
        "Replace category with the mean of the target variable for that category.\n",
        "\n",
        "For example, if predicting test scores and:\n",
        "\n",
        "\"Public\" schools average score = 70\n",
        "\n",
        "\"Private=8\n",
        "Then replace \"Public\" â†’ 70, \"Private\" â†’ 85"
      ],
      "metadata": {
        "id": "PMRTRxuUwajr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "Ans. **Training Dataset**\n",
        "\n",
        "* This is a part of the data we **use to train the model.**\n",
        "* The model looks at the input features and correct answers to **learn patterns.**\n",
        "* It adjusts itself to reduce errors- this process is called **training**.\n",
        "\n",
        "**Testing Dataset**\n",
        "\n",
        "* This is **separate data** that model has **never seen before.**\n",
        "* After training, we uuse this data to evaluate how well the model performs.\n",
        "* It helps us check if the model is generalizing well or just memorizing the training data."
      ],
      "metadata": {
        "id": "ZCrSQOXhxjhC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "Ans. sklearn.preprocessing is a **module in Scikit-learn** (a popular Python library for ML) that contains tools to **prepare your data before training** a model.\n",
        "\n",
        "Basically, it helps you **clean, scale, encode, or transform** your features so your model can learn more effectively.\n",
        "\n",
        "**Why is Preprocessing Important?**\n",
        "\n",
        "Many machine learning algorithms:\n",
        "\n",
        "* Donâ€™t like missing values\n",
        "\n",
        "* Expect numerical input\n",
        "\n",
        "* Perform better when data is scaled or normalized\n",
        "\n",
        "So, preprocessing makes sure your data is **in the right format and scale.**\n",
        "\n",
        "**Common Tools sklearn.preprocessing:**\n",
        "\n",
        "1. StandardScaler\n",
        "2. MinMaxScaler\n",
        "3. LabelEncoder\n",
        "4. OneHotEncoder\n",
        "5. Binarizer\n",
        "6. PolynomialFeatures"
      ],
      "metadata": {
        "id": "2QDRZRVtzDC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is a Test set?\n",
        "\n",
        "Ans. A **test set** is a portion of your dataset that you **set aside to evaluate your machine learning model** after it has been trained.\n",
        "\n",
        "It contains **unseen data** â€” meaning the model has never looked at it during training.\n",
        "\n",
        "We use a test set to:\n",
        "\n",
        "* **Check how well the model performs** on new, real-world data\n",
        "\n",
        "* See if the model is **generalizing** (learning patterns) instead of just memorizing training data\n",
        "\n",
        "* Avoid **overfitting** (when a model is great on training data but bad on new data)\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine you're building a model to predict student grades based on study hours.\n",
        "\n",
        "You might:\n",
        "\n",
        "* Use 80 students' data to train the model (training set)\n",
        "\n",
        "* Use 20 students' data to test the model (test set)\n",
        "\n",
        "If the model predicts grades well for the test students, it's probably working well!\n",
        "\n"
      ],
      "metadata": {
        "id": "4WFUa43p0OsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing ) in Python?\n",
        "   \n",
        "   How do you approach  a Machine Learning problem?\n",
        "\n",
        "Ans. Splitting data into **training and testing** sets is a crucial part of building machine learning models. In Python, we can do this easily using **scikit-learn's** train_test_split function. Let me show you how:\n",
        "\n",
        "**Steps to Split Data in Python(Using scikit-learn):**\n",
        "\n",
        "1. **Import Neccessary Libraries**: First , you need to import the necessary libraries and modules.\n",
        "\n",
        "2. **Prepare Your Data** Let us assume you have dataset x (features) and y (target labels)\n",
        "\n",
        "3. **Split the data** Use **train_test_split()** to split the dataset into training and testing sets.\n",
        "\n",
        "**Approaching a machine learning problem involves several key steps. Hereâ€™s a general workflow to help guide you:**\n",
        "\n",
        "1. **Define the Problem**\n",
        "\n",
        "Understand the problem you're trying to solve:\n",
        "\n",
        "* Is it a classification problem? (e.g., spam detection, disease diagnosis)\n",
        "\n",
        "* Is it a regression problem? (e.g., predicting house prices, stock prices)\n",
        "\n",
        "2. **Collect Data**\n",
        "\n",
        "Gather the data needed to train your model:\n",
        "\n",
        "* Can be structured (like spreadsheets) or unstructured (like images or text).\n",
        "\n",
        "* Ensure you have enough data to train a reliable model.\n",
        "\n",
        "3. **Preprocess the Data**\n",
        "\n",
        "Data rarely comes in a \"perfect\" format. Preprocessing often includes:\n",
        "\n",
        "* Handling missing values (filling in or removing missing data)\n",
        "\n",
        "* Feature scaling/normalization (to ensure consistency in values across features)\n",
        "\n",
        "* Encoding categorical variables (e.g., converting text labels into numerical values)\n",
        "\n",
        "* Feature engineering (creating new features based on domain knowledge)\n",
        "\n",
        "4. **Split the Data**\n",
        "\n",
        "Split the dataset into:\n",
        "\n",
        "* Training set (used to train the model)\n",
        "\n",
        "* Test set (used to evaluate the modelâ€™s performance on unseen data)\n",
        "\n",
        "* (Optional) Validation set (used during model training to fine-tune parameters)\n",
        "\n",
        "5. **Choose a Model**\n",
        "\n",
        "Pick an appropriate machine learning model based on the problem:\n",
        "\n",
        "* Supervised learning: Decision trees, logistic regression, SVM, k-NN, etc.\n",
        "\n",
        "* Unsupervised learning: K-means clustering, PCA, etc.\n",
        "\n",
        "* Deep learning: Neural networks if the problem is complex (e.g., image classification, language processing)\n",
        "\n",
        "6. **Train the Model**\n",
        "\n",
        "* Train the model using your training data. This means the model learns from the input data to make predictions.\n",
        "\n",
        "7. **Evaluate the Model**\n",
        "\n",
        "After training, evaluate the modelâ€™s performance on the test set (data the model hasn't seen during training). Use appropriate metrics:\n",
        "\n",
        "* Accuracy for classification problems\n",
        "\n",
        "* Mean Squared Error (MSE) for regression problems\n",
        "\n"
      ],
      "metadata": {
        "id": "4vq2qTwt1A6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "Ans. **Exploratory Data Analysis (EDA)** is the process of exploring, visualizing, and understanding your dataset before applying any machine learning models.\n",
        "\n",
        "Itâ€™s like getting to know your data:\n",
        "\n",
        "What kind of data do you have?\n",
        "\n",
        "Are there any patterns, trends, or outliers?\n",
        "\n",
        "Are there errors or missing values?\n",
        "\n",
        "Hereâ€™s why EDA is crucial:\n",
        "\n",
        "1. **Clean and Prepare the Data**\n",
        "\n",
        "EDA helps you spot:\n",
        "\n",
        "Missing values\n",
        "\n",
        "Incorrect data types\n",
        "\n",
        "Duplicates\n",
        "\n",
        "Inconsistent formats\n",
        "\n",
        "You can't build a good model on messy data.\n",
        "\n",
        "2. **Understand Feature Distributions**\n",
        "\n",
        "EDA helps you understand how each variable behaves.\n",
        "\n",
        "For example, are there skewed features? Do you need to apply transformations?\n",
        "\n",
        "Some models (like linear regression) assume normal distributions, so this matters.\n",
        "\n",
        "3. **Identify Categorical vs. Continuous Features**\n",
        "\n",
        "Helps decide how to encode variables (e.g., one-hot encoding, label encoding).\n",
        "\n",
        "Understand if ordinal relationships exist (e.g., Small, Medium, Large).\n",
        "\n",
        "4. **Detect Outliers and Anomalies**\n",
        "\n",
        "Outliers can heavily skew your model and degrade performance.\n",
        "\n",
        "Visualizing data using box plots or scatter plots can help spot them early.\n",
        "\n",
        "5. **Reveal Relationships Between Variables**\n",
        "\n",
        "Use correlation matrices or scatter plots to see how features relate to each other and to the target.\n",
        "\n",
        "This can help with:\n",
        "\n",
        "Feature selection\n",
        "\n",
        "Detecting multicollinearity (two features being highly correlated)\n",
        "\n",
        "Identifying which features might be most useful to the model\n",
        "\n",
        "6. **Understand the Target Variable**\n",
        "\n",
        "Is it balanced or imbalanced? (e.g., 90% yes, 10% no)\n",
        "\n",
        "If it's imbalanced, you might need techniques like SMOTE, undersampling, or class weighting.\n",
        "\n",
        "7. **Guide Your Modeling Choices**\n",
        "\n",
        "EDA gives you clues about:\n",
        "\n",
        "Which model types to try (linear vs. non-linear)\n",
        "\n",
        "What preprocessing is needed\n",
        "\n"
      ],
      "metadata": {
        "id": "7B_eurTo3mUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "Ans. Correlation is a **statistical measure** that describes how **two variables move in relation to each other**.\n",
        "\n",
        "It tells you:\n",
        "\n",
        "* Do they move **together** (both increase or both decrease)?\n",
        "\n",
        "* Do they move in **opposite directions** (one goes up, the other goes down)?\n",
        "\n",
        "* Or do they seem to be **unrelated**?"
      ],
      "metadata": {
        "id": "E07MFKZBwYhL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "Ans. A **negative correlation** means that as **one variable increases, the other decreases** â€” they move in **opposite directions.**\n",
        "\n",
        "**In Simple Terms:**\n",
        "\n",
        "* When X goes up, Y goes down\n",
        "\n",
        "* When X goes down, Y goes up\n",
        "\n",
        "The **correlation coefficient (r)** will be **less than 0**, and closer to -1 means a stronger negative relationship."
      ],
      "metadata": {
        "id": "h-pPOhXw5CKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "Ans. **Step 1: Load Your Data**\n",
        "\n",
        "* First, you load your dataset using a tool like pandas, which helps organize the data in a table (called a DataFrame).\n",
        "\n",
        "* Example: You might have columns like Age, Salary, Education_Level.\n",
        "\n",
        "**Step 2: Select Numeric Variables**\n",
        "\n",
        "* Correlation works on numerical data.\n",
        "\n",
        "* So you focus on columns like age, income, scores â€” anything with numbers.\n",
        "\n",
        "**Step 3: Apply a Correlation Function**\n",
        "\n",
        "* Use a built-in pandas function (like .corr()) that automatically compares all numeric columns.\n",
        "\n",
        "* It calculates the correlation coefficient for each pair of variables.\n",
        "\n",
        "**Step 4: Interpret the Correlation Matrix**\n",
        "\n",
        "* The result is a table (called a correlation matrix) showing:\n",
        "\n",
        "* How strongly each pair of variables is related\n",
        "\n",
        "Whether the correlation is positive, negative, or none\n",
        "\n",
        "**Step 5: (Optional) Visualize with a Heatmap**\n",
        "\n",
        "* You can use a library like seaborn to turn the matrix into a colorful chart.\n",
        "\n",
        "* This helps you spot strong or weak relationships more easily.\n",
        "\n"
      ],
      "metadata": {
        "id": "7qToYvgQ5wJf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "Ans. **Causation** means that **one event is the result of the occurrence of another event** â€” in other words:\n",
        "\n",
        "**A causes B**\n",
        "\n",
        "If there's **causation**, changing one variable will **directly affect** the other.\n",
        "\n",
        "Aspect | Correlation | Causation\n",
        "Meaning | Variables are related | One variable directly affects the other\n",
        "Directionality | No direction (just association) | Has direction: A â†’ B\n",
        "Proof | Observed in data | Requires deeper evidence or experiments\n",
        "Example | Ice cream sales â†‘ & drowning â†‘ | Smoking â†’ Lung cancer"
      ],
      "metadata": {
        "id": "zctkU23d7Aqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "Ans. An **optimizer** in the context of machine learning and deep learning is an algorithm or method used to **adjust the weights and biases** of a neural network to **minimize the loss function**. The main goal of an optimizer is to find the best parameters that reduce the error between predicted outputs and actual outputs.\n",
        "\n",
        "**Types of Optimizers (with Examples)**\n",
        "\n",
        "1. **Gradient Descent (GD)**\n",
        "* Basic Idea: Updates weights by computing gradients for the whole dataset.\n",
        "\n",
        "* Update Rule:\n",
        "\n",
        "ðœƒ\n",
        "=\n",
        "ðœƒ\n",
        "âˆ’\n",
        "ð›¼\n",
        "â‹…\n",
        "âˆ‡\n",
        "ð½\n",
        "(\n",
        "ðœƒ\n",
        ")\n",
        "Î¸=Î¸âˆ’Î±â‹…âˆ‡J(Î¸)\n",
        "where:\n",
        "\n",
        "ðœƒ\n",
        "Î¸: parameters\n",
        "\n",
        "ð›¼\n",
        "Î±: learning rate\n",
        "\n",
        "âˆ‡\n",
        "ð½\n",
        "(\n",
        "ðœƒ\n",
        ")\n",
        "âˆ‡J(Î¸): gradient of the loss function w.r.t. parameters\n",
        "\n",
        "* Pros: Simple and effective for small datasets\n",
        "\n",
        "* Cons: Very slow for large datasets\n",
        "\n",
        "* Example: Training a linear regression model on a small housing dataset.\n",
        "\n",
        "2. **Stochastic Gradient Descent (SGD)**\n",
        "\n",
        "* Basic Idea: Uses one sample at a time to update weights.\n",
        "\n",
        "Update Rule:\n",
        "* Same as GD, but computed for a single data point.\n",
        "\n",
        "* Pros: Faster and can escape local minima\n",
        "\n",
        "* Cons: Noisy updates; can bounce around the minimum\n",
        "\n",
        "* Example: Image classification with real-time data input.\n",
        "\n",
        "3. **Mini-Batch Gradient Descent**\n",
        "\n",
        "* Basic Idea: Combines the benefits of GD and SGD by using a small batch of data.\n",
        "\n",
        "* Batch Size: Typically 32, 64, or 128 samples.\n",
        "\n",
        "* Pros: More stable than SGD, faster than GD\n",
        "\n",
        "* Example: Training a CNN on a large image dataset like CIFAR-10 with batch size 64.\n",
        "\n",
        "4. **Momentum**\n",
        "\n",
        "* Basic Idea: Accelerates SGD by considering past gradients to keep moving in the right direction.\n",
        "\n",
        "* Update Rule:\n",
        "\n",
        "ð‘£\n",
        "=\n",
        "ð›½\n",
        "ð‘£\n",
        "âˆ’\n",
        "ð›¼\n",
        "âˆ‡\n",
        "ð½\n",
        "(\n",
        "ðœƒ\n",
        ")\n",
        "and\n",
        "ðœƒ\n",
        "=\n",
        "ðœƒ\n",
        "+\n",
        "ð‘£\n",
        "v=Î²vâˆ’Î±âˆ‡J(Î¸)andÎ¸=Î¸+v\n",
        "* Pros: Faster convergence, especially on ravines\n",
        "\n",
        "* Example: Faster training of deep networks with sharp curves in the loss surface.\n",
        "\n",
        "5. **RMSprop (Root Mean Square Propagation)**\n",
        "\n",
        "* Basic Idea: Adapts learning rate based on recent gradients.\n",
        "\n",
        "* Uses: Moving average of squared gradients to scale learning rate.\n",
        "\n",
        "* Pros: Handles noisy problems and non-stationary objectives\n",
        "\n",
        "* Example: Recurrent Neural Networks (RNNs) for time series data.\n",
        "\n",
        "6. **Adam (Adaptive Moment Estimation)**\n",
        "\n",
        "* Basic Idea: Combines Momentum and RMSprop.\n",
        "\n",
        "**Keeps track of:**\n",
        "\n",
        "* Mean of gradients (like Momentum)\n",
        "\n",
        "* Variance of gradients (like RMSprop)\n",
        "\n",
        "**Update Rule:**\n",
        "\n",
        "* Uses biased-corrected first and second moments for stability.\n",
        "\n",
        "* Pros: Fast convergence, works well for most problems\n",
        "\n",
        "* Example: Default optimizer for training Transformer models (e.g., BERT, GPT).\n",
        "\n",
        "7. **Adagrad**\n",
        "\n",
        "* Basic Idea: Adapts learning rate based on the frequency of parameters.\n",
        "\n",
        "* Good for: Sparse data\n",
        "\n",
        "* Pros: Good for NLP tasks\n",
        "\n",
        "* Cons: Learning rate decreases too much over time\n",
        "\n",
        "* Example: Text classification using Bag-of-Words model.\n",
        "\n",
        "8. **Adadelta**\n",
        "\n",
        "* Improves upon Adagrad by fixing the diminishing learning rate problem.\n",
        "\n",
        "* Example: Training deep networks on audio/speech data.\n",
        "\n"
      ],
      "metadata": {
        "id": "NQ2B-3fP8sE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is sklearn.linear_model?\n",
        "\n",
        "Ans. **sklearn.linear_model** is a module in scikit-learn (sklearn) â€” a popular Python library for machine learning. This module contains a collection of linear models used for regression and classification tasks.\n",
        "\n",
        "**What is a Linear Model?**\n",
        "\n",
        "A linear model makes predictions by computing a linear combination of input features:\n",
        "\n",
        "ð‘¦\n",
        "^\n",
        "=\n",
        "ð‘¤\n",
        "1\n",
        "ð‘¥\n",
        "1\n",
        "+\n",
        "ð‘¤\n",
        "2\n",
        "ð‘¥\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ð‘¤\n",
        "ð‘›\n",
        "ð‘¥\n",
        "ð‘›\n",
        "+\n",
        "ð‘\n",
        "y\n",
        "^\n",
        "â€‹\n",
        " =w\n",
        "1\n",
        "â€‹\n",
        " x\n",
        "1\n",
        "â€‹\n",
        " +w\n",
        "2\n",
        "â€‹\n",
        " x\n",
        "2\n",
        "â€‹\n",
        " +â‹¯+w\n",
        "n\n",
        "â€‹\n",
        " x\n",
        "n\n",
        "â€‹\n",
        " +b\n",
        "ð‘¤\n",
        "w: weights (coefficients)\n",
        "\n",
        "ð‘¥\n",
        "x: input features\n",
        "\n",
        "ð‘\n",
        "b: bias (intercept)\n",
        "\n"
      ],
      "metadata": {
        "id": "-MB19lWpIebr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What does model.fit() do? What arguments must be given?\n",
        "\n",
        "Ans. The .fit() method is **one of the most important methods** in scikit-learn models. Itâ€™s the method that **trains the model** by finding the best parameters (like weights and intercept) based on the data you give it.\n",
        "\n",
        "**Required Arguments**\n",
        "\n",
        "1. **X â€“ Input Features**\n",
        "\n",
        "* Type: array-like, shape (n_samples, n_features)\n",
        "\n",
        "* This is your data/input matrix (e.g., rows of data, each with several features)\n",
        "\n",
        "2. **y â€“ Target Values (Labels)**\n",
        "\n",
        "* Type: array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
        "\n",
        "* These are the outputs you want the model to learn to predict.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SNmBpHt-I3PQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. What does model.predict() do? What arguments must be given?\n",
        "\n",
        "Ans. The .predict() method is used to **make predictions** using a model that has already been trained with .fit().\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "**You give it new input data (X), and it gives you the predicted output (Å·).**\n",
        "\n",
        "**Required Argument**\n",
        "1. **X â€“ Input Features**\n",
        "\n",
        "* Type: array-like, shape (n_samples, n_features)\n",
        "\n",
        "* This should have the same number of features as the data you used to train the model.\n",
        "\n",
        "**What It Returns:**\n",
        "\n",
        "* An array of predicted values, shape (n_samples,) for most cases.\n",
        "\n",
        "* For regression: returns continuous values\n",
        "\n",
        "* For classification: returns class labels\n",
        "\n"
      ],
      "metadata": {
        "id": "ytYdqBUcJf-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are continuos and categorical variables?\n",
        "\n",
        "Ans. **Continuous Variables**\n",
        "\n",
        "**Definition:**\n",
        "\n",
        "A **continuous variable is a numeric variable** that can take **any value within a range**. It can be infinitely divided, meaning values can have **decimals** or fractions.\n",
        "\n",
        " **Examples:**\n",
        "\n",
        "* Height (e.g., 172.5 cm)\n",
        "\n",
        "* Temperature (e.g., 36.6Â°C)\n",
        "\n",
        "* Income (e.g., $45,000.75)\n",
        "\n",
        "**Categorical Variables**\n",
        "\n",
        "* **Definition:**\n",
        "A categorical variable represents groups or categories. These variables have a limited number of distinct values, and the values represent labels, not quantities.\n",
        "\n",
        "**Types of Categorical Variables:**\n",
        "\n",
        "1. **Nominal** â€“ No natural order\n",
        "â†’ Examples:\n",
        "\n",
        "* Gender (Male, Female, Other)\n",
        "\n",
        "* Color (Red, Blue, Green)\n",
        "\n",
        "* Animal type (Dog, Cat, Bird)\n",
        "\n",
        "2. **Ordinal** â€“ Have a meaningful order\n",
        "â†’ Examples:\n",
        "\n",
        "* Size (Small, Medium, Large)\n",
        "\n",
        "* Rating (Bad, Average, Good, Excellent)\n",
        "\n"
      ],
      "metadata": {
        "id": "6fL9d0QaKJ7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "Ans. **Feature scaling** is the process of **normalizing or standardizing the range of independent variables (features)** so that they all contribute equally to the learning process.\n",
        "\n",
        "**Why Is It Important in Machine Learning?**\n",
        "\n",
        "1. **Improves Model Performance**\n",
        "\n",
        "Some algorithms (like KNN, SVM, Logistic Regression) rely on distance or gradient calculations.\n",
        "\n",
        "Without scaling, large-valued features dominate the learning process.\n",
        "\n",
        "2. **Speeds Up Convergence**\n",
        "\n",
        "Optimizers (like Gradient Descent) converge faster when features are on similar scales.\n",
        "\n",
        "3. **Avoids Bias**\n",
        "\n",
        "Ensures that no single feature disproportionately influences the model just because it has a bigger scale."
      ],
      "metadata": {
        "id": "bc0Up8Y4LMV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. How do we perform scaling in Python?\n",
        "\n",
        "Ans. Scaling in Python typically refers to **feature scaling**â€”a data preprocessing step used in machine learning and data analysis to standardize the range of independent variables. Here's how you can perform **scaling** using common libraries like scikit-learn.\n",
        "\n",
        "**Common Scaling Techniques in Python**\n",
        "\n",
        "1. **Standardization(Z-Score normalization)**\n",
        "\n",
        "2. **Min-Max Scaling**\n",
        "\n",
        "3. **MaxAbs Scaling**\n",
        "\n",
        "**When to Use What?**\n",
        "\n",
        "* **StandardScaler**: Good for algorithms that assume normally distributed data (e.g., linear regression, logistic regression).\n",
        "\n",
        "* **MinMaxScaler**: Best when you need a bounded range (e.g., neural networks).\n",
        "\n",
        "* **RobustScaler**: Best when your data has outliers.\n",
        "\n",
        "* **MaxAbsScaler**: Good for sparse data.\n",
        "\n"
      ],
      "metadata": {
        "id": "PCSsgtyk8lUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "Ans. **sklearn.preprocessing** is a **module in scikit-learn** (a popular machine learning library in Python) that provides a bunch of **utility functions and classes** for preparing your data before feeding it into a machine learning model.\n",
        "\n",
        "It helps with data transformation, which includes:\n",
        "\n",
        "* **Feature Scaling**\n",
        "\n",
        "* StandardScaler â€“ standardizes features by removing the mean and scaling to unit variance.\n",
        "\n",
        "* MinMaxScaler â€“ scales features to a specific range, usually [0, 1].\n",
        "\n",
        "* RobustScaler â€“ scales features using statistics that are robust to outliers.\n",
        "\n",
        "* MaxAbsScaler â€“ scales data to the range [-1, 1] without shifting/centering the data.\n",
        "\n",
        "* **Encoding Categorical Data**\n",
        "\n",
        "* OneHotEncoder â€“ converts categorical variables into a one-hot numeric array.\n",
        "\n",
        "* OrdinalEncoder â€“ encodes categorical features as integers.\n",
        "\n",
        "* LabelEncoder â€“ encodes target labels with values between 0 and n_classes-1 (used for target variable, not features).\n",
        "\n",
        "* **Normalization**\n",
        "\n",
        "* Normalizer â€“ scales input vectors individually to unit norm (used often in text or signal processing).\n",
        "\n",
        "* **Binarization**\n",
        "\n",
        "* Binarizer â€“ converts numerical values into 0s and 1s based on a threshold.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iWHWBKzYXaHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?"
      ],
      "metadata": {
        "id": "-x7H9_6hYS8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'feature1': [1, 3, 5, 7],\n",
        "    'feature2': [2, 4, 6, 8],\n",
        "    'label': [0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "X = df[['feature1', 'feature2']]\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "X8JH9Ma1YrcF",
        "outputId": "cfab6c73-d655-4184-8789-433a9fdd5b31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b6b846aaa2cc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "Ans. **Data encoding** is the process of converting **categorical (non-numeric)** data into a numerical format so that machine learning models can understand and process it.\n",
        "\n",
        "ML models can only work with **numbers**, so if your dataset has strings like \"red\", \"blue\", or \"male\", \"female\", you need to encode them into numbers first.\n",
        "\n",
        "**Common Types of Encoding**\n",
        "\n",
        "1. **Label Encoding**\n",
        "\n",
        "2. **One-Hot Encoding**\n",
        "\n",
        "3. **Ordinal Encoding**"
      ],
      "metadata": {
        "id": "xNZBw-3nYn2j"
      }
    }
  ]
}